{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My plan so far after experimenting with the data:\n",
    "\n",
    "drop all cols except for make, model, eventid, event date, amateur built, weather condition, engine type, and the injury totals.\n",
    "\n",
    "Clean the data using as much of the code I have already written as possible. Define functions early for operations I think will be commmon so I don't have to keep copy pasting them.\n",
    "\n",
    "group by both make and model in the main df. Make separate DFs that all additionally aggregate based on injury totals, as well as one of amateur built, weather condition, and engine type.\n",
    "\n",
    "create a statistic that represents the liklihood of any one make an model to be in an accident vs another. Then, add in the comparison of whether they are amateur built, the weather conditions at the time of the incident, and the engine type to see how it changes the distribution.\n",
    "\n",
    "recommend whether or not amateur built aircraft are safe compared to others\n",
    "\n",
    "recommend specific makes and models that perform well in bad weather conditions and single out those that do not perform\n",
    "\n",
    "recommend planes that have a specific engine type that performs well?\n",
    "\n",
    "make a recommendation of what the best overall aircraft is based on all of these analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: import packages and read the CSV into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Event.Id</th>\n",
       "      <th>Investigation.Type</th>\n",
       "      <th>Accident.Number</th>\n",
       "      <th>Event.Date</th>\n",
       "      <th>Location</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Airport.Code</th>\n",
       "      <th>Airport.Name</th>\n",
       "      <th>...</th>\n",
       "      <th>Purpose.of.flight</th>\n",
       "      <th>Air.carrier</th>\n",
       "      <th>Total.Fatal.Injuries</th>\n",
       "      <th>Total.Serious.Injuries</th>\n",
       "      <th>Total.Minor.Injuries</th>\n",
       "      <th>Total.Uninjured</th>\n",
       "      <th>Weather.Condition</th>\n",
       "      <th>Broad.phase.of.flight</th>\n",
       "      <th>Report.Status</th>\n",
       "      <th>Publication.Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20001218X45444</td>\n",
       "      <td>Accident</td>\n",
       "      <td>SEA87LA080</td>\n",
       "      <td>1948-10-24</td>\n",
       "      <td>MOOSE CREEK, ID</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Personal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>UNK</td>\n",
       "      <td>Cruise</td>\n",
       "      <td>Probable Cause</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20001218X45447</td>\n",
       "      <td>Accident</td>\n",
       "      <td>LAX94LA336</td>\n",
       "      <td>1962-07-19</td>\n",
       "      <td>BRIDGEPORT, CA</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Personal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>UNK</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Probable Cause</td>\n",
       "      <td>19-09-1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20061025X01555</td>\n",
       "      <td>Accident</td>\n",
       "      <td>NYC07LA005</td>\n",
       "      <td>1974-08-30</td>\n",
       "      <td>Saltville, VA</td>\n",
       "      <td>United States</td>\n",
       "      <td>36.922223</td>\n",
       "      <td>-81.878056</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Personal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IMC</td>\n",
       "      <td>Cruise</td>\n",
       "      <td>Probable Cause</td>\n",
       "      <td>26-02-2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20001218X45448</td>\n",
       "      <td>Accident</td>\n",
       "      <td>LAX96LA321</td>\n",
       "      <td>1977-06-19</td>\n",
       "      <td>EUREKA, CA</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Personal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>IMC</td>\n",
       "      <td>Cruise</td>\n",
       "      <td>Probable Cause</td>\n",
       "      <td>12-09-2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20041105X01764</td>\n",
       "      <td>Accident</td>\n",
       "      <td>CHI79FA064</td>\n",
       "      <td>1979-08-02</td>\n",
       "      <td>Canton, OH</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Personal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>VMC</td>\n",
       "      <td>Approach</td>\n",
       "      <td>Probable Cause</td>\n",
       "      <td>16-04-1980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Event.Id Investigation.Type Accident.Number  Event.Date  \\\n",
       "0  20001218X45444           Accident      SEA87LA080  1948-10-24   \n",
       "1  20001218X45447           Accident      LAX94LA336  1962-07-19   \n",
       "2  20061025X01555           Accident      NYC07LA005  1974-08-30   \n",
       "3  20001218X45448           Accident      LAX96LA321  1977-06-19   \n",
       "4  20041105X01764           Accident      CHI79FA064  1979-08-02   \n",
       "\n",
       "          Location        Country   Latitude   Longitude Airport.Code  \\\n",
       "0  MOOSE CREEK, ID  United States        NaN         NaN          NaN   \n",
       "1   BRIDGEPORT, CA  United States        NaN         NaN          NaN   \n",
       "2    Saltville, VA  United States  36.922223  -81.878056          NaN   \n",
       "3       EUREKA, CA  United States        NaN         NaN          NaN   \n",
       "4       Canton, OH  United States        NaN         NaN          NaN   \n",
       "\n",
       "  Airport.Name  ... Purpose.of.flight Air.carrier Total.Fatal.Injuries  \\\n",
       "0          NaN  ...          Personal         NaN                  2.0   \n",
       "1          NaN  ...          Personal         NaN                  4.0   \n",
       "2          NaN  ...          Personal         NaN                  3.0   \n",
       "3          NaN  ...          Personal         NaN                  2.0   \n",
       "4          NaN  ...          Personal         NaN                  1.0   \n",
       "\n",
       "  Total.Serious.Injuries Total.Minor.Injuries Total.Uninjured  \\\n",
       "0                    0.0                  0.0             0.0   \n",
       "1                    0.0                  0.0             0.0   \n",
       "2                    NaN                  NaN             NaN   \n",
       "3                    0.0                  0.0             0.0   \n",
       "4                    2.0                  NaN             0.0   \n",
       "\n",
       "  Weather.Condition  Broad.phase.of.flight   Report.Status Publication.Date  \n",
       "0               UNK                 Cruise  Probable Cause              NaN  \n",
       "1               UNK                Unknown  Probable Cause       19-09-1996  \n",
       "2               IMC                 Cruise  Probable Cause       26-02-2007  \n",
       "3               IMC                 Cruise  Probable Cause       12-09-2000  \n",
       "4               VMC               Approach  Probable Cause       16-04-1980  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### import packages\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "### load csv into dataframe\n",
    "df = pd.read_csv('data/Aviation_Data.csv', low_memory=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: remove as many uncessary columns as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### drop all of the columns I do not plan to use in my analysis\n",
    "df=df.drop(axis=1, labels=\n",
    "        ['Investigation.Type', 'Accident.Number', 'Location', 'Country', 'Latitude', 'Longitude', 'Airport.Code', 'Airport.Name', 'Aircraft.damage', 'Aircraft.Category', 'Registration.Number', 'FAR.Description', 'Schedule', 'Purpose.of.flight', 'Air.carrier', 'Broad.phase.of.flight', 'Publication.Date']\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Event.Id                   1459\n",
       "Event.Date                 1459\n",
       "Injury.Severity            2459\n",
       "Make                       1522\n",
       "Model                      1551\n",
       "Amateur.Built              1561\n",
       "Number.of.Engines          7543\n",
       "Engine.Type                8536\n",
       "Total.Fatal.Injuries      12860\n",
       "Total.Serious.Injuries    13969\n",
       "Total.Minor.Injuries      13392\n",
       "Total.Uninjured            7371\n",
       "Weather.Condition          5951\n",
       "Report.Status              7840\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### find total number of NaN entries in each column\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    88813\n",
       "True      1535\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### find total number of duplicate rows\n",
    "df.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: remove all duplicate records from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    88813\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### drop all duplicate rows and confirm there are no more duplicates\n",
    "df = df.drop_duplicates()\n",
    "df.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: comb through the remaining columns and sort out NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### While it would definitely be easier to just run the .dropna method on the entire dataframe to sort all of these out\n",
    "### immediately, I determined there were a few special cases in the data that I want to handle manually during my exploration.\n",
    "### Even though it is inefficient, I will be combing through the columns and sorting out the missing values manually.\n",
    "df['Event.Id'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Executing order 66 on all NaNs in the event.id column\n",
    "df=df.dropna(subset=['Event.Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Event.Id                      0\n",
       "Event.Date                    0\n",
       "Injury.Severity             999\n",
       "Make                         63\n",
       "Model                        92\n",
       "Amateur.Built               102\n",
       "Number.of.Engines          6077\n",
       "Engine.Type                7074\n",
       "Total.Fatal.Injuries      11390\n",
       "Total.Serious.Injuries    12492\n",
       "Total.Minor.Injuries      11916\n",
       "Total.Uninjured            5907\n",
       "Weather.Condition          4491\n",
       "Report.Status              6379\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### confirming we got rid of the event.id NaN entries. It looks like the NaN entries in event.date just\n",
    "### happened to coincide with the NaN entires of Event.Id, saving me some work!\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Non-Fatal      67309\n",
       "Fatal(1)        6161\n",
       "Fatal           5262\n",
       "Fatal(2)        3705\n",
       "Incident        2211\n",
       "Fatal(3)        1145\n",
       "Fatal(4)         807\n",
       "Fatal(5)         235\n",
       "Minor            217\n",
       "Serious          173\n",
       "Fatal(6)         161\n",
       "Unavailable       96\n",
       "Fatal(7)          56\n",
       "Fatal(8)          51\n",
       "Fatal(10)         32\n",
       "Fatal(9)          18\n",
       "Fatal(14)         11\n",
       "Fatal(11)         10\n",
       "Fatal(13)          9\n",
       "Fatal(12)          8\n",
       "Name: Injury.Severity, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### The Injury severity column isn't missing too many values, but the data is extremely irregular and hard to work with for the\n",
    "### purpose of my analysis, and is made superfluous by the inclusion of the four injury type columns. I think this column should\n",
    "### only have two values: fatal and non-fatal, so manipulating it into this form is my next goal.\n",
    "\n",
    "df['Injury.Severity'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Non-Fatal    69910\n",
       "Fatal(1)      6161\n",
       "Fatal         5262\n",
       "Fatal(2)      3705\n",
       "Fatal(3)      1145\n",
       "Fatal(4)       807\n",
       "Fatal(5)       235\n",
       "Fatal(6)       161\n",
       "Fatal(7)        56\n",
       "Fatal(8)        51\n",
       "Fatal(10)       32\n",
       "Fatal(9)        18\n",
       "Fatal(14)       11\n",
       "Fatal(11)       10\n",
       "Fatal(13)        9\n",
       "Fatal(12)        8\n",
       "Fatal(18)        5\n",
       "Fatal(20)        5\n",
       "Fatal(15)        5\n",
       "Fatal(25)        4\n",
       "Name: Injury.Severity, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### This cell standardizes the non-fatal values, removes NaNs, and drops a placeholder value that I found.\n",
    "\n",
    "df['Injury.Severity'] = df['Injury.Severity'].replace(to_replace=['Incident', 'Minor', 'Serious'], value='Non-Fatal')\n",
    "df = df.drop(df[df['Injury.Severity'] == 'Unavailable'].index)\n",
    "df.dropna(axis=0, subset=['Injury.Severity'], inplace=True)\n",
    "df['Injury.Severity'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is the cell that standardizes the different types of fatal entry under a single string. It just looks to see if the\n",
    "### first five characters of the entry are 'fatal' and if they are, it sets the entry to 'fatal'.\n",
    "\n",
    "df['Injury.Severity'] = df['Injury.Severity'].map(lambda x: 'Fatal' if x[:5]=='Fatal' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Non-Fatal    69910\n",
       "Fatal        17807\n",
       "Name: Injury.Severity, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Showing that the above code performs the operation I set out to do\n",
    "\n",
    "df['Injury.Severity'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cessna               22194\n",
       "Piper                12010\n",
       "CESSNA                4841\n",
       "Beech                 4324\n",
       "PIPER                 2814\n",
       "Bell                  2113\n",
       "Boeing                1529\n",
       "Mooney                1092\n",
       "Grumman               1091\n",
       "BEECH                 1025\n",
       "Robinson               941\n",
       "Bellanca               886\n",
       "Hughes                 793\n",
       "BOEING                 720\n",
       "Schweizer              627\n",
       "Air Tractor            594\n",
       "BELL                   570\n",
       "Mcdonnell Douglas      515\n",
       "Aeronca                486\n",
       "Maule                  445\n",
       "Name: Make, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Now I need to handle the make and model columns. They are both categorical and do not appear to have any placeholder values,\n",
    "### so I just drop their NaN values after having a look at them.\n",
    "\n",
    "df['Make'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=0, subset=['Make'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152          2353\n",
       "172          1742\n",
       "172N         1161\n",
       "PA-28-140     930\n",
       "150           825\n",
       "172M          797\n",
       "172P          685\n",
       "182           656\n",
       "180           622\n",
       "150M          585\n",
       "PA-18         579\n",
       "PA-18-150     578\n",
       "PA-28-180     571\n",
       "PA-28-161     568\n",
       "PA-28-181     528\n",
       "206B          520\n",
       "PA-38-112     467\n",
       "150L          461\n",
       "G-164A        460\n",
       "A36           450\n",
       "Name: Model, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Model'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=0, subset=['Model'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Event.Id                      0\n",
       "Event.Date                    0\n",
       "Injury.Severity               0\n",
       "Make                          0\n",
       "Model                         0\n",
       "Amateur.Built                89\n",
       "Number.of.Engines          5247\n",
       "Engine.Type                6178\n",
       "Total.Fatal.Injuries      11291\n",
       "Total.Serious.Injuries    12388\n",
       "Total.Minor.Injuries      11813\n",
       "Total.Uninjured            5809\n",
       "Weather.Condition          3512\n",
       "Report.Status              5441\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Checking the results of that last operation\n",
    "\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No     79111\n",
       "Yes     8427\n",
       "Name: Amateur.Built, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Amateur.Built'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    79111\n",
       "True      8427\n",
       "Name: Amateur.Built, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### This column seems like it would be easier to work with if the data was in boolean format. The code below drops the NaN values\n",
    "### and manipulates it into that format by mapping 'yes' to true and anything else to false.\n",
    "df.dropna(subset=['Amateur.Built'], inplace=True)\n",
    "df['Amateur.Built'] = df['Amateur.Built'].map(lambda x: True if x=='Yes' else False)\n",
    "df['Amateur.Built'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0    69428\n",
      "2.0    10831\n",
      "0.0     1204\n",
      "3.0      482\n",
      "4.0      411\n",
      "8.0        3\n",
      "6.0        1\n",
      "Name: Number.of.Engines, dtype: int64\n",
      "1.1438805245264692\n"
     ]
    }
   ],
   "source": [
    "print(df['Number.of.Engines'].value_counts().head(10))\n",
    "print(df['Number.of.Engines'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Event.Id                      0\n",
       "Event.Date                    0\n",
       "Injury.Severity               0\n",
       "Make                          0\n",
       "Model                         0\n",
       "Amateur.Built                 0\n",
       "Number.of.Engines             0\n",
       "Engine.Type                6110\n",
       "Total.Fatal.Injuries      11235\n",
       "Total.Serious.Injuries    12309\n",
       "Total.Minor.Injuries      11730\n",
       "Total.Uninjured            5750\n",
       "Weather.Condition          3464\n",
       "Report.Status              5441\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### I plan to use the number of engines in my analysis, but I don't think it's important enough to me to delete the roughly 6% \n",
    "### of total records that have a NaN for this column. It doesn't really make sense to fill it with the mean either, because \n",
    "### this column is exclusively integer values, and the mean would be a float. Instead, I think it makes the most sense to just\n",
    "### fill the NaNs in this column with the median.\n",
    "\n",
    "df['Number.of.Engines'].fillna(df['Number.of.Engines'].median(), inplace=True)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0    74606\n",
      "2.0    10831\n",
      "0.0     1204\n",
      "3.0      482\n",
      "4.0      411\n",
      "8.0        3\n",
      "6.0        1\n",
      "Name: Number.of.Engines, dtype: int64\n",
      "1.1353697822659874\n"
     ]
    }
   ],
   "source": [
    "### Checking to see how that operation affected the data. It looks like it very slightly lowered the mean value by assuming\n",
    "### all the NaNs were planes with 1 engine, which should be perfectly fine for my analysis. \n",
    "print(df['Number.of.Engines'].value_counts().head(10))\n",
    "print(df['Number.of.Engines'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reciprocating    0.852385\n",
      "Turbo Shaft      0.044125\n",
      "Turbo Prop       0.041288\n",
      "Turbo Fan        0.028786\n",
      "Unknown          0.024439\n",
      "Turbo Jet        0.008560\n",
      "None             0.000233\n",
      "Electric         0.000098\n",
      "NONE             0.000025\n",
      "LR               0.000025\n",
      "Name: Engine.Type, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "### Now, it's time to work with the Engine Type data. Judging by the outcome of the value counts calls below and the number of\n",
    "### NaNs in the column, it is going to be hard to use this data without significant effort to clean it.\n",
    "print(df['Engine.Type'].value_counts(normalize=True).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reciprocating      69408\n",
       "Turbo Shaft         3593\n",
       "Turbo Prop          3362\n",
       "Turbo Fan           2344\n",
       "Unknown             1990\n",
       "Turbo Jet            697\n",
       "None                  19\n",
       "Electric               8\n",
       "NONE                   2\n",
       "LR                     2\n",
       "UNK                    1\n",
       "Geared Turbofan        1\n",
       "Hybrid Rocket          1\n",
       "Name: Engine.Type, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Engine.Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reciprocating      0.792890\n",
       "Unknown            0.092771\n",
       "Turbo Shaft        0.041045\n",
       "Turbo Prop         0.038406\n",
       "Turbo Fan          0.026777\n",
       "Turbo Jet          0.007962\n",
       "Electric           0.000091\n",
       "LR                 0.000023\n",
       "UNK                0.000011\n",
       "Geared Turbofan    0.000011\n",
       "Hybrid Rocket      0.000011\n",
       "Name: Engine.Type, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### My strategy to clean the data is as follows: First, I will simply drop the none values, since they are such a small subset\n",
    "### of the total. Then, I will fill the NaNs with the 'unknown' placeholder value. After that, I will randomly assign values to\n",
    "### the unknown values according to the normal distribution of the categorical data, NOT including outliers with only 1-2 values. \n",
    "### (This is an idea I'm taking from a solution branch in a previous lab, but I'm rewriting it to be tailored for my specific\n",
    "### purpose, based on my own understanding of how it works. Note to self: as the instructor if the code I came up with here is ok.)\n",
    "df['Engine.Type'] = df['Engine.Type'].loc[df['Engine.Type'] != 'NONE']\n",
    "df['Engine.Type'] = df['Engine.Type'].loc[df['Engine.Type'] != 'None']\n",
    "df['Engine.Type'].fillna(value='Unknown', inplace=True)\n",
    "df['Engine.Type'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7928899449381983,\n",
       " 0.041045031871872785,\n",
       " 0.03840617788845987,\n",
       " 0.026776942584934543,\n",
       " 0.007962256391509973]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Grab the current normal distribution of the values by converting normalized value counts to a dict and\n",
    "### assigning the values to a list variable. Then, remove extreme outliers and Unknowns.\n",
    "\n",
    "norms = dict(df['Engine.Type'].value_counts(normalize=True))\n",
    "remove_list = ['Unknown', 'Electric', 'LR', 'UNK', 'Geared Turbofan', 'Hybrid Rocket']\n",
    "for key in remove_list:\n",
    "    del norms[key]\n",
    "norms = list(norms.values())\n",
    "norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8741121354087955,\n",
       " 0.045249609591456345,\n",
       " 0.042340436250062966,\n",
       " 0.029519923429550145,\n",
       " 0.008777895320135004]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Now that I have a list of normalized values for each of the remaining entries in this column, I need to convert these values\n",
    "### into probabilities that I can instruct numpy to consider while it is assigning new values to unknowns randomly. To do this,\n",
    "### I need to determine the relative percentage of each of these values to their sum.\n",
    "\n",
    "probabilities = []\n",
    "for number in norms:\n",
    "    probabilities.append(number/sum(norms))\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reciprocating      0.873826\n",
       "Turbo Shaft        0.045557\n",
       "Turbo Prop         0.042462\n",
       "Turbo Fan          0.029210\n",
       "Turbo Jet          0.008796\n",
       "Electric           0.000091\n",
       "LR                 0.000023\n",
       "UNK                0.000011\n",
       "Geared Turbofan    0.000011\n",
       "Hybrid Rocket      0.000011\n",
       "Name: Engine.Type, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Now that I have the list of probabilities for each value I want to fill the Unknowns with, I can finally write the call\n",
    "### that will replace the unknowns for me. I am simply going to write a lambda function to do this, since I only plan on doing it\n",
    "### once.\n",
    "\n",
    "df['Engine.Type'] = df['Engine.Type'].map(lambda x: np.random.choice(['Reciprocating', 'Turbo Shaft', 'Turbo Prop', 'Turbo Fan', 'Turbo Jet'], p=probabilities) if x=='Unknown' else x)\n",
    "df['Engine.Type'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Event.Id                      0\n",
       "Event.Date                    0\n",
       "Injury.Severity               0\n",
       "Make                          0\n",
       "Model                         0\n",
       "Amateur.Built                 0\n",
       "Number.of.Engines             0\n",
       "Engine.Type                   0\n",
       "Total.Fatal.Injuries      11235\n",
       "Total.Serious.Injuries    12309\n",
       "Total.Minor.Injuries      11730\n",
       "Total.Uninjured            5750\n",
       "Weather.Condition          3464\n",
       "Report.Status              5441\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### It looks like that worked exactly how I wanted it to! This definitely has an affect on the summary statistics of this\n",
    "### column, but it's a more acceptable effect than just dropping over 5% of the values would be. Next, I'll check the \n",
    "### amount of NaNs in the data again.\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    0.767663\n",
       "1.0    0.115998\n",
       "2.0    0.067494\n",
       "3.0    0.020668\n",
       "4.0    0.014364\n",
       "Name: Total.Fatal.Injuries, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Next on the list are the counts of individuals who were injured/uninjured in each incident. These have a lot of NaN values\n",
    "### for some reason. The grand majority of incidents did not have any fatalities, and many of the incidents that did have fatalities\n",
    "### have a unique number of fatalities. For this reason, I don't think it's reasonable to fill the NaNs with the normal distribution\n",
    "### of values, as we may assign a 0 to an incident that has a 'fatal' value in the Injury.Severity column.\n",
    "\n",
    "### My strategy to handle this column is to first test if there are any cases where Injury.Severity says there is a fatality but\n",
    "### Total.Fatal.Injuries has a NaN value. If it does not, it should be perfectly fine for me to just fill the NaNs with 0s.\n",
    "df['Total.Fatal.Injuries'].value_counts(normalize=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = df.loc[df['Injury.Severity'] == 'Fatal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Event.Id                     0\n",
       "Event.Date                   0\n",
       "Injury.Severity              0\n",
       "Make                         0\n",
       "Model                        0\n",
       "Amateur.Built                0\n",
       "Number.of.Engines            0\n",
       "Engine.Type                  0\n",
       "Total.Fatal.Injuries         0\n",
       "Total.Serious.Injuries    2615\n",
       "Total.Minor.Injuries      2914\n",
       "Total.Uninjured           2947\n",
       "Weather.Condition         1507\n",
       "Report.Status             2232\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Looks like there are no cases that meet the criteria that I set out, so I am just going to fill that column's NaNs with 0s.\n",
    "df['Total.Fatal.Injuries'].fillna(value=0.0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Event.Id                      0\n",
       "Event.Date                    0\n",
       "Injury.Severity               0\n",
       "Make                          0\n",
       "Model                         0\n",
       "Amateur.Built                 0\n",
       "Number.of.Engines             0\n",
       "Engine.Type                   0\n",
       "Total.Fatal.Injuries          0\n",
       "Total.Serious.Injuries    12309\n",
       "Total.Minor.Injuries      11730\n",
       "Total.Uninjured            5750\n",
       "Weather.Condition          3464\n",
       "Report.Status              5441\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The next order of business is to figure out the serious and minor injury columns. I am unconvinced that I actually need these\n",
    "### two columns; I feel like the total number of fatal injuries and the total number of uninjured passengers in combination with \n",
    "### the injury.severity column is probably already enough information for my analysis. For now, I'm just\n",
    "### going to drop those two columns to save myself the headache of figuring out how to deal with each of them having 10-12% of\n",
    "### values as NaN.\n",
    "df=df.drop(axis=1, labels=['Total.Serious.Injuries', 'Total.Minor.Injuries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    0.352326\n",
       "1.0    0.306573\n",
       "2.0    0.194980\n",
       "3.0    0.052575\n",
       "4.0    0.032474\n",
       "5.0    0.010808\n",
       "6.0    0.006089\n",
       "7.0    0.003399\n",
       "8.0    0.001956\n",
       "9.0    0.001553\n",
       "Name: Total.Uninjured, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Now I need to deal with the Total.Uninjured column. Roughly 7% of all entries have NaN in this column, and I'm not comfortable\n",
    "### dropping that many values. I think in this case I'd like to just fill with the median value and call it a day, because accuracy\n",
    "### in this specific category is not vitally important to the accuracy of my analysis. \n",
    "\n",
    "df['Total.Uninjured'].value_counts(normalize=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    0.352121\n",
       "0.0    0.329183\n",
       "2.0    0.182172\n",
       "3.0    0.049122\n",
       "4.0    0.030341\n",
       "5.0    0.010098\n",
       "6.0    0.005689\n",
       "7.0    0.003176\n",
       "8.0    0.001828\n",
       "9.0    0.001451\n",
       "Name: Total.Uninjured, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Total.Uninjured'].fillna(df['Total.Uninjured'].median(), inplace=True)\n",
    "df['Total.Uninjured'].value_counts(normalize=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VMC    0.916597\n",
       "IMC    0.070735\n",
       "UNK    0.009848\n",
       "Unk    0.002819\n",
       "Name: Weather.Condition, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### That caused a significant change in the distribution where 1.0 became the most common value, but the other effects on the\n",
    "### the distribution seem pretty minor to me. I'm happy to leave it like this at the moment, and will revisit to apply the \n",
    "### same method I used on the engine.type column earlier if I decide greater accuracy is necessary. Now, I'll investigate the\n",
    "### last two columns.\n",
    "df['Weather.Condition'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VMC    0.929265\n",
       "IMC    0.070735\n",
       "Name: Weather.Condition, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### I think this column will play an important role in my analysis, so I want to preserve the original data as much as I possibly can.\n",
    "### I will drop the NaNs, and for the unknowns, I plan to just assign them to VMC because it is more common. The bonus of this\n",
    "### is that it will preserve the cases with IMC weather conditions, which will help me to analyze which makes and models perform\n",
    "### best under poor weather conditions accurately - High visibility condition cases are of little use to me. \n",
    "\n",
    "df.dropna(axis=0, subset=['Weather.Condition'], inplace=True)\n",
    "df.loc[df['Weather.Condition'] == 'UNK', 'Weather.Condition'] = 'VMC'\n",
    "df.loc[df['Weather.Condition'] == 'Unk', 'Weather.Condition'] = 'VMC'\n",
    "df['Weather.Condition'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Probable Cause                                                                                                                                                                                                                                                                                                                                                 61568\n",
       "Foreign                                                                                                                                                                                                                                                                                                                                                         1462\n",
       "Factual                                                                                                                                                                                                                                                                                                                                                          135\n",
       "The pilot's failure to maintain directional control during the landing roll.                                                                                                                                                                                                                                                                                      58\n",
       "A loss of engine power for undetermined reasons.                                                                                                                                                                                                                                                                                                                  53\n",
       "                                                                                                                                                                                                                                                                                                                                                               ...  \n",
       "The pilot's in-flight loss of control due to his failure to correctly connect the elevator control cables during the restoration/rebuild of the airplane. Contributing to this accident was the pilot's decision to perform a high-speed taxi test prior to having the airplane inspected by a certificated mechanic, which resulted in inadvertent flight.        1\n",
       "The pilot's decision to execute a precautionary landing to unsuitable terrain.                                                                                                                                                                                                                                                                                     1\n",
       "The pilot's inadvertent encounter with a mound while landing on a remote airstrip.                                                                                                                                                                                                                                                                                 1\n",
       "Excessive engine operating temperature, which resulted in engine failure and a forced landing. The reason for the excessive engine operating temperature could not be determined during postaccident engine examination.                                                                                                                                           1\n",
       "The failure of company maintenance personnel to install the upper pivot bolt through the pivot hole in the upper end of the landing gear actuating rod, and the company maintenance inspector's inadequate inspection of the work performed.                                                                                                                       1\n",
       "Name: Report.Status, Length: 16873, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### I'm happy with the accuracy of this result. Time to check the last Column!\n",
    "df['Report.Status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Event.Id                0\n",
       "Event.Date              0\n",
       "Injury.Severity         0\n",
       "Make                    0\n",
       "Model                   0\n",
       "Amateur.Built           0\n",
       "Number.of.Engines       0\n",
       "Engine.Type             0\n",
       "Total.Fatal.Injuries    0\n",
       "Total.Uninjured         0\n",
       "Weather.Condition       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### This column is extremely interesting. It may help us to gain some insight into the specific failures that caused fatal\n",
    "### accidents. Unfortunately, the data is extremely irregular in nature and contains missing values. We can't really use it\n",
    "### for concrete statistical analysis, so I think it's definitely best just to give it the drop.\n",
    "df = df.drop(axis=1, labels='Report.Status')\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
